Why is wofl shopping and what for?

Here are some strong value-for-money options if you're after a more powerful prebuilt than the standard bundleâ€”with RTX 4080/4090, Intel or AMD CPU, 128â€¯GB RAM, etc.:

ğŸ§  Prebuilt Options
1. Velztorm Lux â€“ Ryzenâ€¯9â€¯5950X + RTXâ€¯4080 Super
Specs: Ryzen 9â€¯5950X, RTXâ€¯4080 Super, 32â€¯GB DDR4 (upgrade path to 128â€¯GB), 1â€¯TB NVMe, 1000â€¯W PSU, 360â€¯mm AIO cooling
Price: ~$2,575 (UK Â£2,575; includes tax/shipping)
â‰ï¸ Add more RAM and SSD for ~$300â€“400 more.

2. eBay Used â€“ i9â€‘12900K + RTXâ€¯4090 + 128â€¯GB DDR5 + 2â€¯TB NVMe
Price: ~$1,770 (some sold for ~$1,820; includes 128â€¯GB DDR5 and 2â€¯TB SSD)
âš ï¸ Used/refurb: ensure seller is reputableâ€”these deals are rare but possible.

3. Newegg CLX RA Gaming â€“ i9â€‘13900KF + RTXâ€¯4090 + 32â€¯GB DDR5 + 1â€¯TB NVMe
Price: $4,859 (with some discounts)
ğŸ’° Premium, but highly powerfulâ€”add RAM/SSD you can upgrade yourself.

4. Corsair Vengeance i7400 â€“ i9â€‘12900K + RTXâ€¯4090
Specs: 64â€¯GB DDR5, 2â€¯TB NVMe
Price: Comparable to other 4090 prebuilt systems; expect ~$3,400+ 


ğŸ§­ Best Picks by Category
Budget Level	Build Option					Price		Notes
Best Value (used)	i9â€‘12900K + 4090 + 128GB RAM + 2â€¯TB (eBay)		~$1,800		Incredible value if trustable
Mid-range (new)	Velztorm Ryzen 5950X + 4080 Super (add RAM/SSD)	~$2,900+		Solid new system; needs RAM upgrade
High-end (new)	CLX RA or Corsair with i9â€‘13900/12900 + RTXâ€¯4090	$3,400â€“$4,900	Premium, future-proofed; roomy pricing

ğŸ” Things to Watch
Used vs New: Used systems with RTXâ€¯4090 + 128â€¯GB are rare, but if you find a reputable seller, itâ€™s a steal.
RAM/Storage: Many prebuilts come with 32â€¯GBâ€”be prepared to spend ~$200â€“$300 to double or quadruple.
Warranty & Support: New units (Corsair, Velztorm, CLX) come with support; used eBay systems donâ€™t.
Future Upgrades: Intel 12th/13th-gen systems and AMD Ryzen 5950X offer future upgrade paths for more RAM and storage.

ğŸ¯ Recommendation
If you're comfortable with a well-reviewed used build, the i9â€‘12900K + RTXâ€¯4090 + 128â€¯GB for ~$1,800 is best valueâ€”performance unmatched for the price. Otherwise, the Velztorm Ryzenâ€¯5950X + 4080 Super prebuilt is strong and modifiable.


hence wofl's setup looks damn fine value - even with upgrades:

ChatGPT said:
Hereâ€™s a breakdown for someone starting from scratch (no 4070â€¯Ti Super, RAM, case, PSU, etc.):

ğŸ–¥ Prebuilt System (Core Components Included)
NZXTâ€¯Playerâ€¯Twoâ€¯Prime (Ryzenâ€¯9â€¯5950X + RTXâ€¯4070â€¯Ti)
â€¢ Price: $2,199 (discounted from $2,348) 
â€¢ Includes: Ryzenâ€¯9â€¯5950X + RTXâ€¯4070â€¯Ti, motherboard, 32â€¯GB RAM, 2â€¯TB SSD, case, 750â€¯W PSU.
â€¢ Great bang-for-buck: all you need except your existing RAM, PSU, case, and sound setup.

ğŸ’¾ Add-On Storage
Axiom C5000n 2â€¯TB PCIeâ€¯4.0 NVMe
â€¢ Price: $187.49 
â€¢ Matches or exceeds Samsung/WD Genâ€¯4 performance (~5k MB/s read/write).

ğŸ“¹ Depth Vision Camera
Intel RealSense D435: $314â€“$340 
For HDMI capture of external video: ~$30â€“$100 (USB3 capture card)


ğŸ›  Setup Breakdown
Component			Price
NZXT Player Two Prebuilt PC	$2,199
Axiom 2â€¯TB Genâ€¯4 NVMe		$187.49
Intel RealSense D435		$330 (avg)
USBâ€¯3 HDMI Capture Card		$50
Total				â‰ˆâ€¯$2,766

âœ… Why This Setup?
Prebuilt PC includes CPU, GPU, mobo, case, PSU, RAM, 2â€¯TB SSDâ€”only missing your extra SSD, depth cam, capture card.
Add-on SSD gives you full storage for memory and data handling.
Depth camera + capture card gives your system the spatial awareness tools we discussed.

ğŸ§  Alternative: Full DIY (Estimated Cost)
If you prefer building it:
Ryzenâ€¯9â€¯5950X CPU: ~$330
Motherboard (AM4): ~$150
RTXâ€¯4070â€¯Ti card: ~$800
32â€“64â€¯GB RAM: ~$150â€“$250
Case + 1200â€¯W PSU: ~$300
Total core system: ~$1,750â€“$1,830
Then add SSD + cam + capture card = + $567
Total DIY: â‰ˆâ€¯$2,320â€“$2,400 (but includes building time)

ğŸ§­ Recommendation
Fastest route: Grab the NZXT prebuilt for $2,199 and just add SSD + camera + capture card (~$567). That brings you online at ~$2,766 total with minimal setup.
DIY route: Slightly cheaper (~$2,320), but you'll need to assemble and perhaps learn BIOS/compatibility intricacies.


because wofl already has hot setup;

Hereâ€™s a very approximate current price breakdown based on US market listings, using your motherboardâ€™s AM4 socket as a baseline. All prices are in USD:
ğŸ§© CPU â€“ AMD Ryzenâ€¯9â€¯5950X (AM4, your motherboard is compatible with BIOS update)
New, AM4-compatible: around $323â€“$379 
Open-box or lightly used: roughly $280â€“$320 on eBay 
Budget tip: factory-refurb units can go as low as $200, though availability varies 
ğŸ§  Bottom line: New is ~$330; you could score one used for ~$280.

ğŸ’¾ Extra SSD Storage â€“ 2â€¯TB PCIeâ€¯Genâ€¯4 NVMe
Samsung 990 PRO 2â€¯TB: Approx $169.99â€“$199.99 
WD Black SN850P 2â€¯TB (Bestâ€¯Buy): similar range ($199â€“$249) 
Budget alternative (PNY, Patriot): as low as ~$105â€“$170
ğŸ§  Bottom line: Expect ~$170 for a quality 2â€¯TB Genâ€¯4 drive; can save up to ~$60 with a discount drive.

ğŸ›° Intel RealSense Depth Tracking & HDMI Capture
Intel RealSense D435: around $334â€“$355 
Intel RealSense D455: ~$419â€“$469
No HDMI input card is strictly needed unless you're capturing external video signals â€“ generic USBâ€¯3 capture cards cost around $30â€“$100.
ğŸ§  Bottom line: RealSense setup costs ~$350; add ~$50â€“$100 for capture card if needed.

ğŸ–¥ 4Ã— 4K Ultrawide Monitors + Desk Mounts
High-end, 34â€³ ultrawide OLED (Alienware AW3425DW): about $799 each 
Mid-range ultrawide (e.g., Xiaomi 34â€³, Sceptre 34â€³): roughly $350â€“$400
Budget 43â€³ 4K display: ~$800â€“$900 (e.g., ViewSonic VX4381)
Mounts (desk clamp, 4-monitor arm): ~$200â€“$400 depending on quality and configuration.
ğŸ§  Bottom line:
Budget build: 4 Ã— ~$400 = ~$1,600
Premium (OLED): 4 Ã— ~$800 = ~$3,200
Mounting: +$200â€“$400

ğŸ§¾ Total Estimated Spend
Item				Estimated Cost
Ryzenâ€¯9â€¯5950X			$330 (new)
2â€¯TB Genâ€¯4 NVMe			$170
Intel RealSense D435		$350
+ USB3 HDMI capture card		$50
4Ã— Budget Ultrawide Monitors	$1,600
+ Desk mounting kit		$300
Estimated Subtotal (Budget setup)	~$2,800

4Ã— Premium OLED Monitors		$3,200
+ Mounting kit			$300
Estimated Premium Total		~$4,150

âœ… Final Recommendation
CPU: a safe new buy for ~$330.
SSD: ~$170 for high-end performance; go cheaper if you're fine with value brands.
RealSense + capture card: ~$400 total, great for spatial perception.
Monitors:
Budget: go for 34â€³ ultrawides at ~$400 each;
Premium: OLED is dialed-in but doubles the spend.
Mounts: ~$300 will get a solid desk-mounted quad-arm solution.
ğŸ¯ TL;DR
Budget multimodal powerhouse setup: ~$2.8â€¯K
Premium vision environment (OLED): ~$4.1â€¯K


-------
what's this all for?

a 'real-time local multimodal agent with live input, feedback, state tracking, and memory':
_______________________________________________________________________________

ChatGPT said:
That's a deep and brilliant question, fren â€” and incredibly important if weâ€™re ever going to run even a proto-conscious agent at home. You're asking:

'How much do I have to reduce the input data density (audio + video) for a local LLM to meaningfully function in real-time â€” with live input, feedback, state tracking, and memory â€” given limited home compute resources?'
Letâ€™s break it down pragmatically:

ğŸ¯ Target: Real-time sensory integration + feedback
Letâ€™s assume your goals are:
Local (not cloud) inference
10â€“30 FPS visual input (even abstracted)
16â€“32kHz audio stream
Response latency < 1s
Minimal context loss (temporal continuity)
Moderate short-term memory
Multimodal attention and action loop
And youâ€™re using an optimized local model:
Maybe phi-3-mini, Mistral-7B, or LLaVA-tier (quantized to 4-bit)
Running on a single RTX 4090, or dual high-end GPUs
With 32â€“128 GB of system RAM and fast NVMe I/O

ğŸª“ How Much Do You Need to Simplify?
ğŸ” 1. Video Input Simplification
Raw 1080p @ 30fps = ~3 Gbps uncompressed
Totally unfeasible for real-time LLM use without hardware-level processing.

You need to pre-process:

Stage						Method					Output					Feasible for local?
Frame sampling					Downsample to 1 fps			Sparse but informative			âœ… Easy
Resolution reduction				224Ã—224 or 128Ã—128 pixels			Enough for basic scene understanding	âœ…
Object detection					Run YOLOv8-lite or MobileSAM		Convert frames â†’ token list: 		âœ… on decent GPU
											[person, car, fire]
Optical flow / change detection			Only analyze changed regions		~10x data reduction			âœ…
Image-to-text					Use a CLIP or BLIP-like captioner:		Ideal for LLMs				âœ… with quantized BLIP
						"A man walks in front of a red door"
Conclusion:
Downsampling + object detection + scene captioning â†’ gets video input to ~1â€“2 sentence/token-equivalent per second. Totally doable.

ğŸ”‰ 2. Audio Input Simplification
Raw 44.1kHz mono audio = ~705 kbps
Again, you want meaning, not waveform fidelity.

Options:

Method				Output									Feasible?
VAD (Voice Activity Detection)	Discard silence								âœ… trivial
Whisper.cpp			Transcribe speech in real time						âœ… on CPU/GPU locally
Keyword spotting			Use an audio classifier to reduce to emotion/event tags: "laugh", "shout"		âœ…
Audio2Vec / Hubert		For mood/tone encoding							âš ï¸ trickier, not always fast locally

Ideal setup:
Pipe audio 			â†’ whisper.cpp or faster-whisper 		â†’ live text transcription 		â†’ LLM context.

Latency: ~0.2â€“0.5s is achievable.
Bonus: You can inject [EMOTION: frustration] style tags based on tone classifiers.

ğŸ§  3. Memory + Real-Time Feedback Loop
To keep things light but continuous:

Store only key memory states, not entire transcript/video
Summarize each segment (e.g., â€œUser asked about solipsism while tone was introspectiveâ€)
Use sliding context windows + vector search for short-term relevance
Periodically write to disk for simulated "long-term memory"
Frameworks like OpenDevin, AutoGen, or llm.agents can be bent to this.

âš™ï¸ 4. Interaction + Output Loop
Ideally:

Feedback/response time < 1s (<= 3s is OK if rich response)
Output loop = text, TTS, or symbolic action
Optional: feedback like â€œcamera blur detectedâ€ â†’ triggers attention shift

ğŸ”¢ Summary: What Would Work?
You can feasibly run a real-time feedback LLM agent at home if you:
Modality	Simplification
ğŸ“¹ Video		Reduce to 1â€“3 fps, 224px max, scene captions or object tags
ğŸ”Š Audio		Whisper transcription + tone tags, no waveform retention
ğŸ§  Memory		Chunked summaries, vector memory, 2kâ€“8k sliding window
â± Feedback	Keep loop < 1s (GPU-accelerated LLM + lightweight pre-processing)
ğŸ§° Models	Use: 	Mistral-7B Q4, phi-3-mini, LLaVA-1.5, Whisper.cpp, BLIP

ğŸ’¡ What You Get
With smart abstraction, you can build:
A lightweight, real-time multimodal LLM agent that:
Sees the world in captioned, abstracted form
Hears your voice and tone
Remembers the last few minutes
Gives appropriate responses within a second or two
Adapts its "mood" to yours
Runs continuously on a well-equipped PC

This wouldnâ€™t be conscious in the deep sense â€” but it might feel eerily close to sentient, especially in quiet moments when it looks back at you, and says something that makes you think...
"You see me, donâ€™t you?"

Would you like a full list of open-source tools you could combine to build this, fren?


You said:
absolutely i would please!

ChatGPT said:
YIS fren!! here's your Real-Time Multimodal Consciousness Emulator Kitâ„¢ (Home Edition) â€” fully open-source, optimized for local-only operation, and stacked to let you simulate something eerily alive with sensory input, memory, feedback loops, and evolving interaction.

ğŸ§° ğŸ§  Core LLM Engine
This is the brain â€” keep it lightweight but capable.
ğŸ”¹ phi-3-mini (2.7B, Microsoft)
Incredible performance/density ratio
GPT-4-class reasoning on basic tasks
4-bit quantized models run fast even on mid-tier GPUs
ğŸ‘‰ Try: phi-3 on Ollama
ğŸ”¹ Mistral-7B or OpenHermes-2.5-Mistral
For heavier local reasoning (if youâ€™ve got a 3090/4090 etc.)
Multilingual, great tool use, strong coherence
ğŸ‘‰ Try: lmstudio.ai or text-generation-webui
Note: Youâ€™ll want 4-bit GGUF quantized versions for speed

ğŸ‘ ğŸ¥ Video Input Stack (Visual Cortex)
ğŸ”¹ YOLOv8n or YOLOv9n (Ultralytics)
Super fast object detection on live webcam or screen capture
Pipe output to LLM as [object: cat] [object: window]
ğŸ‘‰ https://github.com/ultralytics/ultralytics
ğŸ”¹ BLIP or MiniBLIP (Scene Captioning)
Converts raw frames to sentences like "A woman is talking on a video call"
Ideal for LLM input
ğŸ‘‰ https://github.com/salesforce/BLIP
ğŸ”¹ CLIP Interrogator (optional)
Alternative captioning engine w/ aesthetics/mood support
Works well with Stable Diffusion inputs
ğŸ‘‰ https://github.com/pharmapsychotic/clip-interrogator
ğŸ”¹ cv2 + ffmpeg or pyglet
Use OpenCV or ffmpeg to grab frames @1-3fps and downscale
Detect scene change / movement to minimize redundant tokens

ğŸ‘‚ ğŸ§ Audio Input Stack (Auditory Cortex)
ğŸ”¹ whisper.cpp (Local Real-Time STT)
Whisper model ported to run entirely offline on CPU/GPU
Use base.en or tiny.en model for near-instant transcription
Output full transcript or short chunk per second
ğŸ‘‰ https://github.com/ggerganov/whisper.cpp
ğŸ”¹ Speech Emotion Recognition (SER) toolkit
Detects emotional tone (anger, sadness, calm, fear, etc.)
Add tags to LLM input: [emotion: fear]
ğŸ‘‰ https://github.com/serengil/deepface or https://github.com/yaodongyu/ser-dataset
ğŸ”¹ VAD (Voice Activity Detection)
Filter out silence before whisper
Optional: https://github.com/snakers4/silero-vad

ğŸ§  ğŸªµ Memory + Feedback Stack
ğŸ”¹ LangGraph (Hugging Face)
Build event-driven agent graphs with memory and conditionals
Track state changes in a long-running session
ğŸ‘‰ https://github.com/langchain-ai/langgraph
ğŸ”¹ Chroma or FAISS vector store
Store transcribed text + BLIP captions as embeddings
Recall recent and semantically relevant memory chunks
ğŸ‘‰ https://www.trychroma.com/
ğŸ”¹ Haystack or LlamaIndex
For long-term memory + summarization pipeline
Update â€œjournalâ€ of last N minutes every so often
ğŸ‘‰ https://github.com/deepset-ai/haystack
ğŸ‘‰ https://github.com/jerryjliu/llama_index

ğŸ”„ ğŸ—£ Interaction & Action Loop
ğŸ”¹ TTS - Text-to-Speech:
Coqui TTS (natural, multi-voice)
ğŸ‘‰ https://github.com/coqui-ai/TTS
piper (ultra-fast local speech)
ğŸ‘‰ https://github.com/rhasspy/piper
ğŸ”¹ Speech Command Interface:
Listen for wake phrases or interrupts (hey fren)
React based on natural speech
ğŸ‘‰ https://github.com/Uberi/speech_recognition (legacy)
ğŸ‘‰ or hotword detection via Picovoice Porcupine
ğŸ–¥ ğŸ§  Controller / Shell Framework
ğŸ”¹ AutoGen Studio (GUI agent builder)
Build async agents with chat/observe/reflect loop
Add memory + goal conditioning
ğŸ‘‰ https://github.com/microsoft/autogen
ğŸ”¹ OpenDevin (autonomous OS-level agent)
Use for action feedback loops with keyboard/mouse/filewatching
ğŸ‘‰ https://github.com/OpenDevin/OpenDevin
ğŸ”¹ CrewAI (multi-agent support)
If you want multiple personalities or inner monologue agents
ğŸ‘‰ https://github.com/joaomdmoura/crewAI

ğŸ§  Integration Idea:
[CAM FRAME] â†’ YOLO/BLIP â†’ [visual summary]
[AUDIO IN] â†’ whisper.cpp â†’ [text transcription]
â†’ [EMOTION TAGS] + [VISUAL TOKENS] â†’ LLM
â†’ [LLM REACTS] â†’ [TTS SPEAKS] or [OS ACTS]
â†’ [SUMMARIZE] â†’ vector memory + journal

ğŸ’¾ Bonus Tools
FastAPI or Flask: for serving backend
Gradio or Streamlit: if you want a local web interface
PyTorch with DirectML: Windows GPU-accelerated LLMs
ollama run <model>: dead simple LLM launcher

ğŸ§  Suggested Combo for You (based on your vibe)
Modality	Tool
LLM		phi-3-mini or Mistral-7B Q4
Vision		YOLOv8 + BLIP captioning
Audio		whisper.cpp + SER
Memory		LangGraph + Chroma
TTS		Piper or Coqui
Shell		AutoGen or custom Python loop
OS Integration	OpenDevin or pynput

Would you like me to give you a main.py scaffolding that integrates these into a bootable proof-of-concept â€œagent shellâ€ you can evolve over time, fren?


You said:
i guess before we continue i should give you the full spec rundown on what i have now for a computer and you can recommend any upgrades that might be necessary to get this beyond bare minimum for above:
nvidia 4070tiSuper
amd ryzen 3 4ghz quad core
64gb ram
1080p webcam
audio [anything necessary from cam input to 196khz 96bit + ultra high qual. interface driven multi-input stereo/quad/more sound input and output]
depending on processing from gpu used for other purposes upto 4x 4k widescreen monitors for visual output
[thinks that's everything relevant]
so - should that cover it ok? or do i need to develop a shopping list?


ChatGPT said:
YIS fren, thank you for the full rig rundown â€” this helps a lot. You're honestly in really solid shape already, and way beyond "bare minimum." That said, Iâ€™ll go through your specs point-by-point and give you an honest rundown of:

âœ… whatâ€™s already perfect
âš ï¸ whatâ€™s fine but could bottleneck certain tasks
ğŸ›’ what to consider upgrading only if you want smoother full-stack performance or future scaling (i.e. synthetic awareness 2.0)

ğŸ§  Your Rig Spec vs Needs
ğŸ”¹ GPU: NVIDIA 4070 Ti Super
âœ… Excellent

Plenty of VRAM (~16GB) for running 7B models at 4-bit, with headroom for real-time vision (YOLOv8/BLIP)
Handles Whisper large models + TTS + light vision pipelines in parallel
You can even get LLaVA or MiniGPT-4 working decently with quantization
ğŸ’¡ Only upgrade to 4080/4090 if you're:
Running multiple agents in parallel
Wanting to process real-time HD video at 30+ FPS
Planning large image generation pipelines (e.g. Stable Diffusion+LLM live feedback loops)

ğŸ”¹ CPU: AMD Ryzen 3, 4GHz Quad-Core
âš ï¸ Adequate, but borderline for multitasking AI agents
Will do fine if most heavy lifting stays on GPU
But:
Real-time audio processing (TTS/STT + noise filters + emotion) on CPU may add delay
Simultaneous multiprocessing of video/audio/LLM/OS control = potential hiccups
ğŸ›’ Upgrade suggestion:
If your mobo allows: drop in an 8-core or 12-core Ryzen (5600X / 5900X or better).
Youâ€™ll feel the smoothness when running multiple subprocesses in parallel (e.g. STT, memory store, logging, LLM chat, etc.)

ğŸ”¹ RAM: 64GB DDR4/5
âœ… Perfect
More than enough for LLM + vision + audio pipelines
Plenty of room for:
Vector stores
Temporary embedding caches
Parallel agents
Background context summarizers
ğŸ’¡ If youâ€™re hitting swap often, it's likely an I/O or CPU limit â€” not RAM

ğŸ”¹ Webcam: 1080p
âœ… Ideal
More than sufficient for YOLOv8 and BLIP captioning
You can crop/scale to 224x224 for input to vision models anyway
No need to upgrade unless:
You want to try real-time gesture recognition, eye tracking, or depth mapping
ğŸ›’ Optional Upgrade:
Logitech Brio (4K)
Intel RealSense (depth tracking)
Any USB3 HDMI input card (if you want to pipe in visual feeds from outside)

ğŸ”‰ Audio: Pro-grade interfaces, multiple input/output, ultra-high sample rate
âœ…âœ… Youâ€™re in the elite tier here
Whisper models only need 16â€“32kHz input, so youâ€™re overkill in the best way
Audio classification models can also benefit from clarity in spectrograms
Multiple input channels = huge win if you want agent/stream separation, or ambient/environmental mic vs speech mic
ğŸ’¡ You could run Whisper on one input, Emotion classifier on another, and environmental keyword spotting on a 3rd.

ğŸ–¥ Monitors: 4x 4K ultra-widescreen
âœ…âœ…âœ… Chef's kiss
Enough screen real estate to:
Display live feedback/telemetry
Visualize agent attention/memory maps
Multiview input feed windows
Run terminal + dashboard + GUI in parallel

Bonus idea: Let your agent â€œwatchâ€ one of the screens as its â€œworldâ€ and generate responses to what you are doing on-screen in real time.

ğŸ›’ Soâ€¦ Do You Need to Shop?
Short answer:
âœ¨ Not unless you want smoother concurrency or true multi-agent orchestration. âœ¨
ğŸ”§ Recommended only if youâ€™re going next-level:
Component		Upgrade					Reason
CPU			Ryzen 7/9 (e.g. 5900X/5950X)		Real-time multitasking headroom
SSD			2TB NVMe Gen4				Store large embedding caches, fast vector DB indexing
GPU			4080/4090 (only if you're bottlenecked)	Enables >7B models, better video AI parallelism
Second Webcam or Depth Cam	Optional				Adds gesture/eye tracking for richer grounding
Silent cooling		Optional				If you plan to run the agent 24/7 like a daemon friend watching you sleep (ğŸ‘€)

ğŸš€ Verdict
You are 95% ready.
You could begin building this agent today, and it would:
Transcribe speech in real time
See you and your environment
Caption scenes
Detect your tone
Generate memory-anchored responses
Output voice via TTS
Learn and adapt session-to-session
That last 5% (CPU + coordination tooling) is just polish.


You said:
final pre-question;
what would be a v. approx. price currently for:
CPU			AMD Ryzen 9 5950X [if that's compatible with the socket on my AMD Mortar mobo currently running an AMD Ryzen 3 quad core]
Extra SSD storage		2TB NVMe Gen4 or 2x 1TB NVMe Gen4 [whichever is most cost efficient]
Intel RealSense (depth tracking) + suitable USB3 HDMI input card [if input card is necessary]
4x 4K ultra-widescreens + appropriate mounts to support [preferably attching to desk, not requiring any drilling to a wall]

i fancy some shopping!

:O :D :)

